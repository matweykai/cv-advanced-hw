{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import cv2\n",
    "import xml.etree.ElementTree as ET\n",
    "import os\n",
    "\n",
    "\n",
    "class VOCDataset(Dataset):\n",
    "\n",
    "    def __init__(self, is_train, file_names, base_dir, image_size=448, grid_size=7, num_bboxes=2, num_classes=20):\n",
    "        self.is_train = is_train\n",
    "        self.image_size = image_size\n",
    "\n",
    "        self.S = grid_size\n",
    "        self.B = num_bboxes\n",
    "        self.C = num_classes\n",
    "\n",
    "        mean = [122.67891434, 116.66876762, 104.00698793]\n",
    "        self.mean = np.array(mean, dtype=np.float32)\n",
    "\n",
    "        self.to_tensor = transforms.ToTensor()\n",
    "\n",
    "        self.paths, self.boxes, self.labels = [], [], []\n",
    "\n",
    "        for line in file_names:\n",
    "            label_path = f\"{base_dir}/{'train' if is_train else 'valid'}/{line}.xml\"\n",
    "            image_path = f\"{base_dir}/{'train' if is_train else 'valid'}/{line}.jpg\"\n",
    "            \n",
    "            # Parse XML file in VOC format\n",
    "            tree = ET.parse(label_path)\n",
    "            root = tree.getroot()\n",
    "            \n",
    "            box = []\n",
    "            label = []\n",
    "            \n",
    "            # Extract object information from XML\n",
    "            for obj in root.findall('object'):\n",
    "                name = obj.find('name').text\n",
    "                # Map class names to class indices (you may need to customize this)\n",
    "                class_idx = 0 if name == 'worker' else 1  # Assuming 'worker' is 0, 'pig' is 1\n",
    "                \n",
    "                bbox = obj.find('bndbox')\n",
    "                x1 = int(float(bbox.find('xmin').text))\n",
    "                y1 = int(float(bbox.find('ymin').text))\n",
    "                x2 = int(float(bbox.find('xmax').text))\n",
    "                y2 = int(float(bbox.find('ymax').text))\n",
    "                \n",
    "                box.append([x1, y1, x2, y2])\n",
    "                label.append(class_idx)\n",
    "                \n",
    "                if len(box) > 0:\n",
    "                    self.boxes.append(torch.Tensor(box))\n",
    "                    self.labels.append(torch.LongTensor(label))\n",
    "                    self.paths.append(image_path)\n",
    "\n",
    "\n",
    "        self.num_samples = len(self.paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path = self.paths[idx]\n",
    "        img = cv2.imread(path)\n",
    "        boxes = self.boxes[idx].clone()  # [n, 4]\n",
    "        labels = self.labels[idx].clone()  # [n,]\n",
    "\n",
    "        if self.is_train:\n",
    "            img, boxes = self.random_flip(img, boxes)\n",
    "            img, boxes = self.random_scale(img, boxes)\n",
    "\n",
    "            img = self.random_blur(img)\n",
    "            img = self.random_brightness(img)\n",
    "            img = self.random_hue(img)\n",
    "            img = self.random_saturation(img)\n",
    "\n",
    "            img, boxes, labels = self.random_shift(img, boxes, labels)\n",
    "            img, boxes, labels = self.random_crop(img, boxes, labels)\n",
    "\n",
    "        # # For debug.\n",
    "        # debug_dir = 'tmp/voc_tta'\n",
    "        # os.makedirs(debug_dir, exist_ok=True)\n",
    "        # img_show = img.copy()\n",
    "        # box_show = boxes.numpy().reshape(-1)\n",
    "        # n = len(box_show) // 4\n",
    "        # for b in range(n):\n",
    "        #     pt1 = (int(box_show[4 * b + 0]), int(box_show[4 * b + 1]))\n",
    "        #     pt2 = (int(box_show[4 * b + 2]), int(box_show[4 * b + 3]))\n",
    "        #     cv2.rectangle(img_show, pt1=pt1, pt2=pt2, color=(0, 255, 0), thickness=1)\n",
    "        # cv2.imwrite(os.path.join(debug_dir, 'test_{}.jpg'.format(idx)), img_show)\n",
    "\n",
    "        h, w, _ = img.shape\n",
    "        boxes /= torch.Tensor([[w, h, w, h]]).expand_as(boxes)  # normalize (x1, y1, x2, y2) w.r.t. image width/height.\n",
    "        target = self.encode(boxes, labels)  # [S, S, 5 x B + C]\n",
    "\n",
    "        img = cv2.resize(img, dsize=(self.image_size, self.image_size), interpolation=cv2.INTER_LINEAR)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)  # assuming the model is pretrained with RGB images.\n",
    "        # img = (img - self.mean) / 255.0  # normalize from -1.0 to 1.0.\n",
    "        img = np.ascontiguousarray(img, dtype=np.float32)\n",
    "        img /= 255.0\n",
    "        img = self.to_tensor(img)\n",
    "\n",
    "        return img, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "\n",
    "    def encode(self, boxes, labels):\n",
    "\n",
    "        S, B, C = self.S, self.B, self.C\n",
    "        N = 5 * B + C\n",
    "\n",
    "        target = torch.zeros(S, S, N)\n",
    "        cell_size = 1.0 / float(S)\n",
    "        boxes_wh = boxes[:, 2:] - boxes[:, :2]  # width and height for each box, [n, 2]\n",
    "        boxes_xy = (boxes[:, 2:] + boxes[:, :2]) / 2.0  # center x & y for each box, [n, 2]\n",
    "        for b in range(boxes.size(0)):\n",
    "            xy, wh, label = boxes_xy[b], boxes_wh[b], int(labels[b])\n",
    "\n",
    "            ij = (xy / cell_size).ceil() - 1.0\n",
    "            i, j = int(ij[0]), int(ij[1])  # y & x index which represents its location on the grid.\n",
    "            x0y0 = ij * cell_size  # x & y of the cell left-top corner.\n",
    "            xy_normalized = (xy - x0y0) / cell_size  # x & y of the box on the cell, normalized from 0.0 to 1.0.\n",
    "\n",
    "            # TBM, remove redundant dimensions from target tensor.\n",
    "            # To remove these, loss implementation also has to be modified.\n",
    "            for k in range(B):\n",
    "                S = 5 * k\n",
    "                target[j, i, S:S + 2] = xy_normalized\n",
    "                target[j, i, S + 2:S + 4] = wh\n",
    "                target[j, i, S + 4] = 1.0\n",
    "            target[j, i, 5 * B + label] = 1.0\n",
    "\n",
    "        return target\n",
    "\n",
    "    @staticmethod\n",
    "    def random_flip(img, boxes):\n",
    "        if random.random() < 0.5:\n",
    "            return img, boxes\n",
    "\n",
    "        h, w, _ = img.shape\n",
    "\n",
    "        img = np.fliplr(img)\n",
    "\n",
    "        x1, x2 = boxes[:, 0], boxes[:, 2]\n",
    "        x1_new = w - x2\n",
    "        x2_new = w - x1\n",
    "        boxes[:, 0], boxes[:, 2] = x1_new, x2_new\n",
    "\n",
    "        return img, boxes\n",
    "\n",
    "    @staticmethod\n",
    "    def random_scale(img, boxes):\n",
    "        if random.random() < 0.5:\n",
    "            return img, boxes\n",
    "\n",
    "        scale = random.uniform(0.8, 1.2)\n",
    "        h, w, _ = img.shape\n",
    "        img = cv2.resize(img, dsize=(int(w * scale), h), interpolation=cv2.INTER_LINEAR)\n",
    "        scale_tensor = torch.FloatTensor([[scale, 1.0, scale, 1.0]]).expand_as(boxes)\n",
    "        boxes = boxes * scale_tensor\n",
    "\n",
    "        return img, boxes\n",
    "\n",
    "    @staticmethod\n",
    "    def random_blur(bgr):\n",
    "        if random.random() < 0.5:\n",
    "            return bgr\n",
    "\n",
    "        ksize = random.choice([2, 3, 4, 5])\n",
    "        bgr = cv2.blur(bgr, (ksize, ksize))\n",
    "        return bgr\n",
    "\n",
    "    @staticmethod\n",
    "    def random_brightness(bgr):\n",
    "        if random.random() < 0.5:\n",
    "            return bgr\n",
    "\n",
    "        hsv = cv2.cvtColor(bgr, cv2.COLOR_BGR2HSV)\n",
    "        h, s, v = cv2.split(hsv)\n",
    "        adjust = random.uniform(0.5, 1.5)\n",
    "        v = v * adjust\n",
    "        v = np.clip(v, 0, 255).astype(hsv.dtype)\n",
    "        hsv = cv2.merge((h, s, v))\n",
    "        bgr = cv2.cvtColor(hsv, cv2.COLOR_HSV2BGR)\n",
    "\n",
    "        return bgr\n",
    "\n",
    "    @staticmethod\n",
    "    def random_hue(bgr):\n",
    "        if random.random() < 0.5:\n",
    "            return bgr\n",
    "\n",
    "        hsv = cv2.cvtColor(bgr, cv2.COLOR_BGR2HSV)\n",
    "        h, s, v = cv2.split(hsv)\n",
    "        adjust = random.uniform(0.8, 1.2)\n",
    "        h = h * adjust\n",
    "        h = np.clip(h, 0, 255).astype(hsv.dtype)\n",
    "        hsv = cv2.merge((h, s, v))\n",
    "        bgr = cv2.cvtColor(hsv, cv2.COLOR_HSV2BGR)\n",
    "\n",
    "        return bgr\n",
    "\n",
    "    @staticmethod\n",
    "    def random_saturation(bgr):\n",
    "        if random.random() < 0.5:\n",
    "            return bgr\n",
    "\n",
    "        hsv = cv2.cvtColor(bgr, cv2.COLOR_BGR2HSV)\n",
    "        h, s, v = cv2.split(hsv)\n",
    "        adjust = random.uniform(0.5, 1.5)\n",
    "        s = s * adjust\n",
    "        s = np.clip(s, 0, 255).astype(hsv.dtype)\n",
    "        hsv = cv2.merge((h, s, v))\n",
    "        bgr = cv2.cvtColor(hsv, cv2.COLOR_HSV2BGR)\n",
    "\n",
    "        return bgr\n",
    "\n",
    "    def random_shift(self, img, boxes, labels):\n",
    "        if random.random() < 0.5:\n",
    "            return img, boxes, labels\n",
    "\n",
    "        center = (boxes[:, 2:] + boxes[:, :2]) / 2.0\n",
    "\n",
    "        h, w, c = img.shape\n",
    "        img_out = np.zeros((h, w, c), dtype=img.dtype)\n",
    "        mean_bgr = self.mean[::-1]\n",
    "        img_out[:, :] = mean_bgr\n",
    "\n",
    "        dx = random.uniform(-w * 0.2, w * 0.2)\n",
    "        dy = random.uniform(-h * 0.2, h * 0.2)\n",
    "        dx, dy = int(dx), int(dy)\n",
    "\n",
    "        if dx >= 0 and dy >= 0:\n",
    "            img_out[dy:, dx:] = img[:h - dy, :w - dx]\n",
    "        elif dx >= 0 and dy < 0:\n",
    "            img_out[:h + dy, dx:] = img[-dy:, :w - dx]\n",
    "        elif dx < 0 and dy >= 0:\n",
    "            img_out[dy:, :w + dx] = img[:h - dy, -dx:]\n",
    "        elif dx < 0 and dy < 0:\n",
    "            img_out[:h + dy, :w + dx] = img[-dy:, -dx:]\n",
    "\n",
    "        center = center + torch.FloatTensor([[dx, dy]]).expand_as(center)  # [n, 2]\n",
    "        mask_x = (center[:, 0] >= 0) & (center[:, 0] < w)  # [n,]\n",
    "        mask_y = (center[:, 1] >= 0) & (center[:, 1] < h)  # [n,]\n",
    "        mask = (mask_x & mask_y).view(-1, 1)  # [n, 1], mask for the boxes within the image after shift.\n",
    "\n",
    "        boxes_out = boxes[mask.expand_as(boxes)].view(-1, 4)  # [m, 4]\n",
    "        if len(boxes_out) == 0:\n",
    "            return img, boxes, labels\n",
    "        shift = torch.FloatTensor([[dx, dy, dx, dy]]).expand_as(boxes_out)  # [m, 4]\n",
    "\n",
    "        boxes_out = boxes_out + shift\n",
    "        boxes_out[:, 0] = boxes_out[:, 0].clamp_(min=0, max=w)\n",
    "        boxes_out[:, 2] = boxes_out[:, 2].clamp_(min=0, max=w)\n",
    "        boxes_out[:, 1] = boxes_out[:, 1].clamp_(min=0, max=h)\n",
    "        boxes_out[:, 3] = boxes_out[:, 3].clamp_(min=0, max=h)\n",
    "\n",
    "        labels_out = labels[mask.view(-1)]\n",
    "\n",
    "        return img_out, boxes_out, labels_out\n",
    "\n",
    "    def random_crop(self, img, boxes, labels):\n",
    "        if random.random() < 0.5:\n",
    "            return img, boxes, labels\n",
    "\n",
    "        center = (boxes[:, 2:] + boxes[:, :2]) / 2.0\n",
    "\n",
    "        h_orig, w_orig, _ = img.shape\n",
    "        h = random.uniform(0.6 * h_orig, h_orig)\n",
    "        w = random.uniform(0.6 * w_orig, w_orig)\n",
    "        y = random.uniform(0, h_orig - h)\n",
    "        x = random.uniform(0, w_orig - w)\n",
    "        h, w, x, y = int(h), int(w), int(x), int(y)\n",
    "\n",
    "        center = center - torch.FloatTensor([[x, y]]).expand_as(center)  # [n, 2]\n",
    "        mask_x = (center[:, 0] >= 0) & (center[:, 0] < w)  # [n,]\n",
    "        mask_y = (center[:, 1] >= 0) & (center[:, 1] < h)  # [n,]\n",
    "        mask = (mask_x & mask_y).view(-1, 1)  # [n, 1], mask for the boxes within the image after crop.\n",
    "\n",
    "        boxes_out = boxes[mask.expand_as(boxes)].view(-1, 4)  # [m, 4]\n",
    "        if len(boxes_out) == 0:\n",
    "            return img, boxes, labels\n",
    "        shift = torch.FloatTensor([[x, y, x, y]]).expand_as(boxes_out)  # [m, 4]\n",
    "\n",
    "        boxes_out = boxes_out - shift\n",
    "        boxes_out[:, 0] = boxes_out[:, 0].clamp_(min=0, max=w)\n",
    "        boxes_out[:, 2] = boxes_out[:, 2].clamp_(min=0, max=w)\n",
    "        boxes_out[:, 1] = boxes_out[:, 1].clamp_(min=0, max=h)\n",
    "        boxes_out[:, 3] = boxes_out[:, 3].clamp_(min=0, max=h)\n",
    "\n",
    "        labels_out = labels[mask.view(-1)]\n",
    "        img_out = img[y:y + h, x:x + w, :]\n",
    "\n",
    "        return img_out, boxes_out, labels_out\n",
    "\n",
    "\n",
    "def test():\n",
    "    from torch.utils.data import DataLoader\n",
    "\n",
    "    base_dir = 'data'\n",
    "\n",
    "    import os\n",
    "    \n",
    "    train_names = [f.rsplit('.jpg', 1)[0] for f in os.listdir(base_dir + '/train') if f.endswith('.jpg')]\n",
    "\n",
    "    dataset = VOCDataset(is_train=True, file_names=train_names, base_dir=base_dir)\n",
    "    data_loader = DataLoader(dataset, batch_size=1, shuffle=False, num_workers=0)\n",
    "\n",
    "    data_iter = iter(data_loader)\n",
    "    for i in range(100):\n",
    "        img, target = next(data_iter)\n",
    "        print(img.size(), target.size())\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import config\n",
    "\n",
    "class Loss(nn.Module):\n",
    "    def __init__(self, fs=config.S, nb=config.B, nc=config.C, lambda_coord=5.0, lambda_noobj=0.5):\n",
    "        super(Loss, self).__init__()\n",
    "\n",
    "        self.FS = fs\n",
    "        self.NB = nb\n",
    "        self.NC = nc\n",
    "        self.lambda_coord = lambda_coord\n",
    "        self.lambda_noobj = lambda_noobj\n",
    "\n",
    "    def compute_iou(self, box1, box2):\n",
    "\n",
    "        def box_area(box):\n",
    "            # box = 4xn\n",
    "            return (box[2] - box[0]) * (box[3] - box[1])\n",
    "\n",
    "        area1 = box_area(box1.T)\n",
    "        area2 = box_area(box2.T)\n",
    "\n",
    "        inter = (torch.min(box1[:, None, 2:], box2[:, 2:]) - torch.max(box1[:, None, :2], box2[:, :2])).clamp(0).prod(2)\n",
    "        return inter / (area1[:, None] + area2 - inter)\n",
    "\n",
    "    def forward(self, prediction, target_tensor):\n",
    "\n",
    "        S, B, C = self.FS, self.NB, self.NC\n",
    "        N = 5 * B + C  # 5=len([x, y, w, h, conf]\n",
    "\n",
    "        batch_size = prediction.size(0)\n",
    "        coord_mask = target_tensor[:, :, :, 4] > 0\n",
    "        noobj_mask = target_tensor[:, :, :, 4] == 0\n",
    "        coord_mask = coord_mask.unsqueeze(-1).expand_as(target_tensor)\n",
    "        noobj_mask = noobj_mask.unsqueeze(-1).expand_as(target_tensor)\n",
    "\n",
    "        coord_pred = prediction[coord_mask].view(-1, N)\n",
    "        bbox_pred = coord_pred[:, :5 * B].contiguous().view(-1, 5)\n",
    "        class_pred = coord_pred[:, 5 * B:]\n",
    "\n",
    "        coord_target = target_tensor[coord_mask].view(-1, N)\n",
    "        bbox_target = coord_target[:, :5 * B].contiguous().view(-1, 5)\n",
    "        class_target = coord_target[:, 5 * B:]\n",
    "\n",
    "        # Compute loss for the cells with no object bbox.\n",
    "        noobj_pred = prediction[noobj_mask].view(-1, N)\n",
    "        noobj_target = target_tensor[noobj_mask].view(-1, N)\n",
    "        noobj_conf_mask = torch.zeros(noobj_pred.size(), dtype=torch.bool)\n",
    "        for b in range(B):\n",
    "            noobj_conf_mask[:, 4 + b * 5] = 1\n",
    "        noobj_pred_conf = noobj_pred[noobj_conf_mask]\n",
    "        noobj_target_conf = noobj_target[noobj_conf_mask]\n",
    "        loss_noobj = F.mse_loss(noobj_pred_conf, noobj_target_conf, reduction='sum')\n",
    "\n",
    "        # Compute loss for the cells with objects.\n",
    "        coord_response_mask = torch.zeros(bbox_target.size(), dtype=torch.bool)\n",
    "        coord_not_response_mask = torch.ones(bbox_target.size(), dtype=torch.bool)\n",
    "        bbox_target_iou = torch.zeros(bbox_target.size())\n",
    "\n",
    "        # Choose the predicted bbox having the highest IoU for each target bbox.\n",
    "        for i in range(0, bbox_target.size(0), B):\n",
    "            pred = bbox_pred[i:i + B]\n",
    "            pred_xyxy = torch.zeros(pred.size())\n",
    "\n",
    "            pred_xyxy[:, :2] = pred[:, :2] / float(S) - 0.5 * pred[:, 2:4]\n",
    "            pred_xyxy[:, 2:4] = pred[:, :2] / float(S) + 0.5 * pred[:, 2:4]\n",
    "\n",
    "            target = bbox_target[i]\n",
    "            target = bbox_target[i].view(-1, 5)\n",
    "            target_xyxy = torch.zeros(target.size())\n",
    "\n",
    "            target_xyxy[:, :2] = target[:, :2] / float(S) - 0.5 * target[:, 2:4]\n",
    "            target_xyxy[:, 2:4] = target[:, :2] / float(S) + 0.5 * target[:, 2:4]\n",
    "\n",
    "            iou = self.compute_iou(pred_xyxy[:, :4], target_xyxy[:, :4])  # [B, 1]\n",
    "            max_iou, max_index = iou.max(0)\n",
    "            max_index = max_index.data\n",
    "\n",
    "            coord_response_mask[i + max_index] = 1\n",
    "            coord_not_response_mask[i + max_index] = 0\n",
    "\n",
    "            bbox_target_iou[i + max_index, 4] = max_iou.data\n",
    "        \n",
    "        # Move tensors to the same device as prediction\n",
    "        device = prediction.device\n",
    "        bbox_target_iou = bbox_target_iou.to(device)\n",
    "        coord_response_mask = coord_response_mask.to(device)\n",
    "        coord_not_response_mask = coord_not_response_mask.to(device)\n",
    "\n",
    "        bbox_pred_response = bbox_pred[coord_response_mask].view(-1, 5)\n",
    "        bbox_target_response = bbox_target[coord_response_mask].view(-1, 5)\n",
    "        target_iou = bbox_target_iou[coord_response_mask].view(-1, 5)\n",
    "\n",
    "        loss_xy = F.mse_loss(bbox_pred_response[:, :2], bbox_target_response[:, :2], reduction='sum')\n",
    "\n",
    "        loss_wh = F.mse_loss(torch.sqrt(bbox_pred_response[:, 2:4]),\n",
    "                             torch.sqrt(bbox_target_response[:, 2:4]), reduction='sum')\n",
    "\n",
    "        loss_obj = F.mse_loss(bbox_pred_response[:, 4], target_iou[:, 4], reduction='sum')\n",
    "\n",
    "        loss_class = F.mse_loss(class_pred, class_target, reduction='sum')\n",
    "\n",
    "        # Total loss\n",
    "        loss = self.lambda_coord * (loss_xy + loss_wh) + loss_obj + self.lambda_noobj * loss_noobj + loss_class\n",
    "        loss = loss / float(batch_size)\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import config\n",
    "\n",
    "def pad(k, p):\n",
    "    if p is None:\n",
    "        p = k // 2\n",
    "    return p\n",
    "\n",
    "\n",
    "class Conv(nn.Module):\n",
    "    def __init__(self, c1, c2, k, s=1, p=None, d=1, g=1, act=True):\n",
    "        super(Conv, self).__init__()\n",
    "        self.conv = nn.Conv2d(c1, c2, k, s, pad(k, p), dilation=d, groups=g, bias=False)\n",
    "        self.bn = nn.BatchNorm2d(c2, momentum=0.03, eps=1e-3) # TODO: check that it is in paper\n",
    "        self.act = nn.LeakyReLU(0.01, inplace=True) if act else nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.act(self.bn(self.conv(x)))\n",
    "\n",
    "\n",
    "class GlobalAvgPool2d(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GlobalAvgPool2d, self).__init__()\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(x):\n",
    "        return torch.mean(x.view(x.size(0), x.size(1), -1), dim=2)\n",
    "\n",
    "\n",
    "class Flatten(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Flatten, self).__init__()\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(x):\n",
    "        return x.view(x.size(0), -1)\n",
    "\n",
    "\n",
    "\n",
    "class Backbone(nn.Module):\n",
    "    def __init__(self, num_classes=config.C, init_weight=True):\n",
    "        super().__init__()\n",
    "\n",
    "        self.features = nn.Sequential(\n",
    "            Conv(3, 64, 7, 2),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            \n",
    "            Conv(64, 192, 3),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            \n",
    "            Conv(192, 128, 1),\n",
    "            Conv(128, 256, 3),\n",
    "            Conv(256, 256, 1),\n",
    "            Conv(256, 512, 3),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            \n",
    "            Conv(512, 256, 1),\n",
    "            Conv(256, 512, 3),\n",
    "            Conv(512, 256, 1),\n",
    "            Conv(256, 512, 3),\n",
    "            Conv(512, 256, 1),\n",
    "            Conv(256, 512, 3),\n",
    "            Conv(512, 256, 1),\n",
    "            Conv(256, 512, 3),\n",
    "            Conv(512, 512, 1),\n",
    "            Conv(512, 1024, 3),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            \n",
    "            Conv(1024, 512, 1),\n",
    "            Conv(512, 1024, 3),\n",
    "            Conv(1024, 512, 1),\n",
    "            Conv(512, 1024, 3)\n",
    "        )\n",
    "\n",
    "        layers = [\n",
    "            *self.features,\n",
    "            GlobalAvgPool2d(),\n",
    "            nn.Linear(1024, num_classes)\n",
    "        ]\n",
    "\n",
    "        self.classifier = nn.Sequential(*layers)\n",
    "\n",
    "        if init_weight:\n",
    "            self._initialize_weights()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.classifier(x)\n",
    "    \n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_in', nonlinearity='leaky_relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, 0, 0.01)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "class Head(nn.Module):\n",
    "    def __init__(self, feature_size, num_boxes, num_classes):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv = nn.Sequential(\n",
    "            Conv(1024, 1024, 3),\n",
    "            Conv(1024, 1024, 3, 2),\n",
    "            Conv(1024, 1024, 3),\n",
    "            Conv(1024, 1024, 3)\n",
    "        )\n",
    "\n",
    "        self.detect = nn.Sequential(\n",
    "            Flatten(),\n",
    "            nn.Linear(7 * 7 * 1024, 4096),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.LeakyReLU(0.01, inplace=True),\n",
    "            nn.Linear(4096, feature_size * feature_size * (5 * num_boxes + num_classes)),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.detect(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class YOLOv1(nn.Module):\n",
    "    def __init__(self, fs=config.S, nb=config.B, nc=config.C, pretrained_backbone=False):\n",
    "        super(YOLOv1, self).__init__()\n",
    "\n",
    "        self.FS = fs\n",
    "        self.NB = nb\n",
    "        self.NC = nc\n",
    "        if pretrained_backbone:\n",
    "            self.features = Backbone().features\n",
    "            darknet = Backbone()\n",
    "            darknet = nn.DataParallel(darknet)\n",
    "            src_state_dict = torch.load('model_best.pth.tar')['state_dict']\n",
    "            dst_state_dict = darknet.state_dict()\n",
    "\n",
    "            for k in dst_state_dict.keys():\n",
    "                print('Loading weight of', k)\n",
    "                dst_state_dict[k] = src_state_dict[k]\n",
    "            darknet.load_state_dict(dst_state_dict)\n",
    "            self.features = darknet.module.features\n",
    "        else:\n",
    "            self.features = Backbone().features\n",
    "        self.head = Head(fs, nb, nc)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.head(x)\n",
    "\n",
    "        x = x.view(-1, self.FS, self.FS, 5 * self.NB + self.NC)\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from loaders.data_loader import VOCDataset\n",
    "from model.model import YOLOv1\n",
    "from model.loss import Loss\n",
    "\n",
    "import os\n",
    "import math\n",
    "import tqdm\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import glob\n",
    "\n",
    "# %% cell 5 code\n",
    "# Learning rate scheduling\n",
    "def update_lr(optimizer, epoch, burning_base, burning_exp=4.0, init_lr=0.001, base_lr=0.01):\n",
    "    if epoch == 0:\n",
    "        lr = init_lr + (base_lr - init_lr) * math.pow(burning_base, burning_exp)\n",
    "    elif epoch == 1:\n",
    "        lr = base_lr\n",
    "    elif epoch == 75:\n",
    "        lr = 0.001\n",
    "    elif epoch == 105:\n",
    "        lr = 0.0001\n",
    "    else:\n",
    "        return\n",
    "\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "\n",
    "\n",
    "def get_lr(optimizer):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        return param_group['lr']\n",
    "\n",
    "\n",
    "# %% cell 6 code\n",
    "# Set up parameters\n",
    "base_dir = './data'\n",
    "log_dir = './weights'\n",
    "init_lr = 0.001\n",
    "base_lr = 0.01\n",
    "momentum = 0.9\n",
    "weight_decay = 5.0e-4\n",
    "num_epochs = 135\n",
    "batch_size = 64\n",
    "seed = 42\n",
    "\n",
    "# Create weights directory\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "# Check if GPU devices are available\n",
    "print(f'CUDA DEVICE COUNT: {torch.cuda.device_count()}')\n",
    "\n",
    "# Check for MPS (Apple Silicon) support first, then CUDA, then fall back to CPU\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device('mps')\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "print(f'Using device: {device}')\n",
    "\n",
    "# %% cell 7 code\n",
    "# Load YOLO model\n",
    "net = YOLOv1(pretrained_backbone=False).to(device)\n",
    "if torch.cuda.device_count() > 1:\n",
    "    net = torch.nn.DataParallel(net)\n",
    "\n",
    "accumulate = max(round(64 / batch_size), 1)\n",
    "\n",
    "params = defaultdict()\n",
    "params['weight_decay'] = weight_decay\n",
    "params['weight_decay'] *= batch_size * accumulate / 64\n",
    "\n",
    "pg0, pg1, pg2 = [], [], []\n",
    "for k, v in net.named_modules():\n",
    "    if hasattr(v, 'bias') and isinstance(v.bias, torch.nn.Parameter):\n",
    "        pg2.append(v.bias)\n",
    "    if isinstance(v, torch.nn.BatchNorm2d):\n",
    "        pg0.append(v.weight)\n",
    "    elif hasattr(v, 'weight') and isinstance(v.weight, torch.nn.Parameter):\n",
    "        pg1.append(v.weight)\n",
    "\n",
    "optimizer = torch.optim.SGD(pg0, lr=init_lr, momentum=momentum, nesterov=True)\n",
    "optimizer.add_param_group({'params': pg1, 'weight_decay': params['weight_decay']})\n",
    "optimizer.add_param_group({'params': pg2})\n",
    "\n",
    "# Setup loss\n",
    "criterion = Loss()\n",
    "\n",
    "# %% cell 8 code\n",
    "# Prepare datasets\n",
    "train_names = [f.rsplit('.jpg', 1)[0] for f in os.listdir(base_dir + '/train') if f.endswith('.jpg')]\n",
    "val_names = [f.rsplit('.jpg', 1)[0] for f in os.listdir(base_dir + '/valid') if f.endswith('.jpg')]\n",
    "\n",
    "# Create datasets with specific file lists\n",
    "train_dataset = VOCDataset(\n",
    "    is_train=True,\n",
    "    base_dir=base_dir,\n",
    "    file_names=train_names,\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=min(batch_size, len(train_dataset)), \n",
    "    shuffle=True, \n",
    "    num_workers=4\n",
    ")\n",
    "\n",
    "val_dataset = VOCDataset(\n",
    "    is_train=False, \n",
    "    base_dir=base_dir,\n",
    "    file_names=val_names,\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset, \n",
    "    batch_size=min(batch_size // 2, max(1, len(val_dataset))), \n",
    "    shuffle=False, \n",
    "    num_workers=4\n",
    ")\n",
    "\n",
    "print('Number of training images: ', len(train_dataset))\n",
    "print('Number of validation images: ', len(val_dataset))\n",
    "\n",
    "# %% cell 9 code\n",
    "# Training loop\n",
    "best_val_loss = np.inf\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print('\\n')\n",
    "    print(f'Starting epoch {epoch} / {num_epochs}')\n",
    "\n",
    "    # Training\n",
    "    net.train()\n",
    "    total_loss = 0.0\n",
    "    total_batch = 0\n",
    "    print(('\\n' + '%10s' * 3) % ('epoch', 'loss', 'gpu'))\n",
    "    progress_bar = tqdm.tqdm(enumerate(train_loader), total=len(train_loader))\n",
    "    for i, (images, targets) in progress_bar:\n",
    "        # Update learning rate\n",
    "        update_lr(optimizer, epoch, float(i) / float(len(train_loader) - 1), \n",
    "                 init_lr=init_lr, base_lr=base_lr)\n",
    "        lr = get_lr(optimizer)\n",
    "\n",
    "        # Load data as a batch\n",
    "        batch_size_this_iter = images.size(0)\n",
    "        images, targets = images.to(device), targets.to(device)\n",
    "\n",
    "        # Forward to compute loss\n",
    "        predictions = net(images)\n",
    "        loss = criterion(predictions, targets)\n",
    "        loss_this_iter = loss.item()\n",
    "        total_loss += loss_this_iter * batch_size_this_iter\n",
    "        total_batch += batch_size_this_iter\n",
    "\n",
    "        # Backward to update model weight\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        mem = '%.3gG' % (torch.cuda.memory_reserved() / 1E9 if torch.cuda.is_available() else 0)\n",
    "        s = ('%10s' + '%10.4g' + '%10s') % ('%g/%g' % (epoch + 1, num_epochs), total_loss / (i + 1), mem)\n",
    "        progress_bar.set_description(s)\n",
    "\n",
    "    # Validation\n",
    "    net.eval()\n",
    "    val_loss = 0.0\n",
    "    total_batch = 0\n",
    "\n",
    "    progress_bar = tqdm.tqdm(enumerate(val_loader), total=len(val_loader))\n",
    "    for i, (images, targets) in progress_bar:\n",
    "        # Load data as a batch\n",
    "        batch_size_this_iter = images.size(0)\n",
    "        images, targets = images.to(device), targets.to(device)\n",
    "\n",
    "        # Forward to compute validation loss\n",
    "        with torch.no_grad():\n",
    "            predictions = net(images)\n",
    "        loss = criterion(predictions, targets)\n",
    "        loss_this_iter = loss.item()\n",
    "        val_loss += loss_this_iter * batch_size_this_iter\n",
    "        total_batch += batch_size_this_iter\n",
    "    val_loss /= float(total_batch)\n",
    "\n",
    "    # Save results\n",
    "    save = {'state_dict': net.state_dict()}\n",
    "    torch.save(save, os.path.join(log_dir, 'final.pth'))\n",
    "    if best_val_loss > val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        save = {'state_dict': net.state_dict()}\n",
    "        torch.save(save, os.path.join(log_dir, 'best.pth'))\n",
    "\n",
    "    # Print\n",
    "    print(f'Epoch [{epoch + 1}/{num_epochs}], Val Loss: {val_loss:.4f}, Best Val Loss: {best_val_loss:.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
