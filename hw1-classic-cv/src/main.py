import argparse
import yaml
import os
import pandas as pd
import torch

# –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ GPU
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"–ò—Å–ø–æ–ª—å–∑—É–µ–º–æ–µ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {DEVICE}")

def load_config(path="configs/config.yaml"):
    """–ó–∞–≥—Ä—É–∑–∫–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –∏–∑ YAML —Ñ–∞–π–ª–∞"""
    with open(path, "r") as f:
        config = yaml.safe_load(f)
    return config

def ensure_result_dir():
    """–°–æ–∑–¥–∞–µ—Ç –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –¥–ª—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤, –µ—Å–ª–∏ –æ–Ω–∞ –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç"""
    if not os.path.exists("result"):
        os.makedirs("result")
    return "result"

def save_duplicates_list(detected, output_dir="result"):
    """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç —Å–ø–∏—Å–æ–∫ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ –≤ CSV —Ñ–∞–π–ª –∏ –≤—ã–≤–æ–¥–∏—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É"""
    ensure_result_dir()
    
    # –°–æ–∑–¥–∞–µ–º –ø—Ä–æ—Å—Ç–æ–π —Å–ø–∏—Å–æ–∫ —Ñ–∞–π–ª–æ–≤-–¥—É–±–ª–∏–∫–∞—Ç–æ–≤
    if isinstance(detected, pd.DataFrame):
        # –ï—Å–ª–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç - DataFrame (–æ—Ç ResNet matcher)
        duplicate_files = detected[detected['IsLeaked'] == 1]['Image'].tolist()
        num_duplicates = len(duplicate_files)
        
        # –°–æ–∑–¥–∞–µ–º —Ñ–æ—Ä–º–∞—Ç —Å–ª–æ–≤–∞—Ä—è, –ø–æ—Ö–æ–∂–∏–π –Ω–∞ –∫–ª–∞—Å—Å–∏—á–µ—Å–∫–∏–π –ø–æ–¥—Ö–æ–¥
        detected_dict = {}
        for filename in duplicate_files:
            detected_dict[filename] = {"match": "–ù–∞–π–¥–µ–Ω–æ –≤ –æ–±—É—á–∞—é—â–µ–º –Ω–∞–±–æ—Ä–µ", "score": 1.0}
        
        # –°–æ–∑–¥–∞–µ–º –¥–∞—Ç–∞—Ñ—Ä–µ–π–º —Å –æ–¥–Ω–æ–π –∫–æ–ª–æ–Ω–∫–æ–π
        df = pd.DataFrame(duplicate_files, columns=['duplicate_filename'])
    else:
        # –ï—Å–ª–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç - —Å–ª–æ–≤–∞—Ä—å (–æ—Ç –∫–ª–∞—Å—Å–∏—á–µ—Å–∫–æ–≥–æ matcher)
        duplicate_files = list(detected.keys())
        num_duplicates = len(duplicate_files)
        
        # –°–æ–∑–¥–∞–µ–º –¥–∞—Ç–∞—Ñ—Ä–µ–π–º —Å –æ–¥–Ω–æ–π –∫–æ–ª–æ–Ω–∫–æ–π
        df = pd.DataFrame(duplicate_files, columns=['duplicate_filename'])
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ CSV
    csv_path = os.path.join(output_dir, "duplicate_files.csv")
    df.to_csv(csv_path, index=False)
    
    # –í—ã–≤–æ–¥–∏–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞—Ö
    if num_duplicates > 0:
        print(f"\n –ù–∞–π–¥–µ–Ω–æ {num_duplicates} –¥—É–±–ª–∏–∫–∞—Ç–æ–≤.")
        print(f"üìÇ –°–ø–∏—Å–æ–∫ —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤ —Ñ–∞–π–ª: {csv_path}")
    else:
        print("\n –î—É–±–ª–∏–∫–∞—Ç—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã.")
    
    return df

def save_groups_list(groups, output_dir="result"):
    """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç —Å–ø–∏—Å–æ–∫ –≥—Ä—É–ø–ø –≤ CSV —Ñ–∞–π–ª –∏ –≤—ã–≤–æ–¥–∏—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É"""
    ensure_result_dir()
    
    # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –≥—Ä—É–ø–ø—ã –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –∏—Ö —Ç–∏–ø–∞
    if isinstance(groups, pd.DataFrame):
        # –ï—Å–ª–∏ —É–∂–µ –¥–∞—Ç–∞—Ñ—Ä–µ–π–º
        df = groups
        num_groups = df['cluster'].nunique() if 'cluster' in df.columns else 0
        total_files = len(df)
    else:
        # –ü–æ–¥—Å—á–µ—Ç –æ–±—â–µ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —Ñ–∞–π–ª–æ–≤ –≤–æ –≤—Å–µ—Ö –≥—Ä—É–ø–ø–∞—Ö
        total_files = sum(len(g) for g in groups)
        num_groups = len(groups)
        
        # –°–æ–∑–¥–∞–µ–º –¥–∞—Ç–∞—Ñ—Ä–µ–π–º –¥–ª—è –≥—Ä—É–ø–ø
        group_data = []
        for group_id, files in enumerate(groups):
            for file in files:
                group_data.append({
                    'group_id': group_id + 1,
                    'filename': file
                })
        
        df = pd.DataFrame(group_data)
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ CSV
    csv_path = os.path.join(output_dir, "grouped_files.csv")
    df.to_csv(csv_path, index=False)
    
    # –í—ã–≤–æ–¥–∏–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞—Ö
    if num_groups > 0:
        print(f"\n –°—Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–æ {num_groups} –≥—Ä—É–ø–ø, —Å–æ–¥–µ—Ä–∂–∞—â–∏—Ö –≤—Å–µ–≥–æ {total_files} —Ñ–∞–π–ª–æ–≤.")
        print(f" –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ñ–∞–π–ª–æ–≤ –ø–æ –≥—Ä—É–ø–ø–∞–º:")
        
        if isinstance(groups, pd.DataFrame):
            group_counts = df.groupby('cluster').size()
            for group_id, count in group_counts.items():
                print(f"  –ì—Ä—É–ø–ø–∞ {group_id + 1}: {count} —Ñ–∞–π–ª–æ–≤")
        else:
            for i, group in enumerate(groups):
                print(f"  –ì—Ä—É–ø–ø–∞ {i+1}: {len(group)} —Ñ–∞–π–ª–æ–≤")
                
        print(f" –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ —Ñ–∞–π–ª: {csv_path}")
    else:
        print("\n –ù–µ —É–¥–∞–ª–æ—Å—å —Å—Ñ–æ—Ä–º–∏—Ä–æ–≤–∞—Ç—å –≥—Ä—É–ø–ø—ã.")
    
    return df

def run_classical(task):
    if task == "match":
        from src.classical.matcher import find_duplicates
        config = load_config()
        train_path = config["paths"]["train_dir"]
        test_path = config["paths"]["test_dir"]
        print("üîç –ó–∞–ø—É—Å–∫ –ø–æ–∏—Å–∫–∞ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ –º–µ—Ç–æ–¥–æ–º classical...")
        detected = find_duplicates(train_path, test_path)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å–ø–∏—Å–æ–∫ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ –∏ –≤—ã–≤–æ–¥–∏–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–µ
        save_duplicates_list(detected)
        return detected
    
    elif task == "group":
        from src.classical.grouper import group_images
        config = load_config()
        train_path = config["paths"].get("train_dir")
        test_path = config["paths"].get("test_dir")
        print(" –ó–∞–ø—É—Å–∫ –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –º–µ—Ç–æ–¥–æ–º classical...")
        result = group_images(train_path, test_path, eps=30, min_samples=2)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≥—Ä—É–ø–ø—ã –∏ –≤—ã–≤–æ–¥–∏–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–µ
        save_groups_list(result)
        return result

def run_resnet(task):
    # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º PyTorch –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –ø–æ–¥—Ö–æ–¥—è—â–µ–≥–æ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞
    if task == "match":
        from src.resnet.matcher import match_resnet
        config = load_config()
        train_path = config["paths"]["train_dir"]
        test_path = config["paths"]["test_dir"]
        print(" –ó–∞–ø—É—Å–∫ –ø–æ–∏—Å–∫–∞ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ –º–µ—Ç–æ–¥–æ–º ResNet...")
        result = match_resnet(train_path, test_path)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å–ø–∏—Å–æ–∫ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ –∏ –≤—ã–≤–æ–¥–∏–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–µ
        save_duplicates_list(result)
        return result
    
    elif task == "group":
        from src.resnet.grouper import group_images
        config = load_config()
        train_path = config["paths"]["train_dir"]
        test_path = config["paths"]["test_dir"]
        print(" –ó–∞–ø—É—Å–∫ –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –º–µ—Ç–æ–¥–æ–º ResNet...")
        result = group_images(train_path, test_path)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≥—Ä—É–ø–ø—ã –∏ –≤—ã–≤–æ–¥–∏–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–µ
        save_groups_list(result)
        return result

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--method", choices=["classical", "resnet"], required=True, help="–ú–µ—Ç–æ–¥ –∞–Ω–∞–ª–∏–∑–∞: classical –∏–ª–∏ resnet")
    parser.add_argument("--task", choices=["match", "group"], required=True, help="–ó–∞–¥–∞—á–∞: match (–ø–æ–∏—Å–∫ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤) –∏–ª–∏ group (–≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞)")
    args = parser.parse_args()

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏–µ –ø–∞–ø–∫–∏ result
    ensure_result_dir()
    
    print(f"–ó–∞–ø—É—Å–∫ –∑–∞–¥–∞—á–∏: {args.task} —Å –º–µ—Ç–æ–¥–æ–º: {args.method}")
    
    try:
        if args.method == "classical":
            run_classical(args.task)
        else:
            run_resnet(args.task)
        
        print("\n –ó–∞–¥–∞—á–∞ —É—Å–ø–µ—à–Ω–æ –∑–∞–≤–µ—Ä—à–µ–Ω–∞. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ –ø–∞–ø–∫—É 'result'.")
    except Exception as e:
        print(f"\n –û—à–∏–±–∫–∞ –ø—Ä–∏ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–∏ –∑–∞–¥–∞—á–∏: {str(e)}")
        import traceback
        traceback.print_exc()